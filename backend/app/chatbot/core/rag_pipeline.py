# backend/app/chatbot/core/rag_pipeline.py
from typing import List
from sqlalchemy.orm import Session
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import logging
import re

from app.chatbot.crud.embedding import get_all_embeddings

logger = logging.getLogger(__name__)

# Sources prioritaires pour le boost
PRIORITY_SOURCES = {"skill", "project", "experience", "education"}

# Cat√©gorisation automatique
CATEGORY_KEYWORDS = {
    "education": [
        r"√©cole", r"dipl[o√¥]me", r"formation", r"universit√©", r"lyc√©e",
        r"master", r"licence", r"bts", r"√©tudes", r"bac"
    ],
    "experience": [
        r"exp√©rience", r"travail", r"poste", r"job", r"stage", r"alternance"
    ],
    "project": [
        r"projet", r"github", r"repo", r"application", r"app", r"site web"
    ],
    "skill": [
        r"comp√©tence", r"skill", r"stack", r"techno", r"langage", r"framework"
    ],
    "identity": [
        r"qui est", r"pr√©sente", r"profil", r"clement gardair"
    ]
}


def detect_category(query: str) -> List[str]:
    """
    D√©tecte automatiquement les cat√©gories pertinentes.
    Combine intelligemment identity + autre cat√©gorie.
    """
    query = query.lower()
    matched = []

    for category, patterns in CATEGORY_KEYWORDS.items():
        for pat in patterns:
            if re.search(pat, query):
                matched.append(category)
                break

    # Si rien d√©tect√© ‚Üí cat√©gorie g√©n√©rale
    if not matched:
        return ["irrelevant"]

    # Si seulement 'identity' ‚Üí on NE FILTRE PAS
    if matched == ["identity"]:
        logger.info("üéØ Identity seule d√©tect√©e ‚Üí aucun filtrage de source")
        return ["irrelevant"]  # => pas de filtre

    # Si identity + autres cat√©gories ‚Üí on retire identity
    if "identity" in matched and len(matched) > 1:
        matched = [c for c in matched if c != "identity"]

    return matched


class RAGPipeline:
    """
    Pipeline RAG optimis√© avec :
    - d√©tection automatique de cat√©gorie
    - filtrage de embeddings par cat√©gorie
    - boost des sources importantes
    """

    def __init__(self, db: Session, embedding_model: str = 'all-MiniLM-L6-v2'):
        self.db = db
        self.embedding_model = SentenceTransformer(embedding_model)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        self.index = faiss.IndexFlatL2(self.embedding_dim)

        # Map ID ‚Üí {content, source}
        self.embeddings_map = {}
        self.id_list = []

        self._load_embeddings_from_db()

    def _load_embeddings_from_db(self):
        embeddings = get_all_embeddings(self.db)

        for emb in embeddings:
            vector = np.array(emb.vector, dtype='float32').reshape(1, -1)
            self.index.add(vector)

            self.embeddings_map[emb.id] = {
                "content": emb.content,
                "source": emb.source,
            }
            self.id_list.append(emb.id)

        logger.info(f"FAISS index reconstruit avec {len(self.embeddings_map)} embeddings")

    def _embed(self, text: str) -> np.ndarray:
        vec = self.embedding_model.encode([text])
        return np.array(vec, dtype='float32')

    def search(self, query: str, top_k: int = 5) -> List[str]:
        """
        Recherche avec :
        - D√©tection de cat√©gorie
        - Filtrage par cat√©gorie
        - Boost des sources importantes
        """
        categories = detect_category(query)
        query_vector = self._embed(query)

        distances, indices = self.index.search(query_vector, top_k * 6)

        logger.info(f"üîç [RAG SEARCH] Query = {query}")
        logger.info(f"Indices FAISS bruts : {indices[0]}")
        logger.info(f"Distances FAISS : {distances[0]}")

        scored_results = []

        for faiss_idx, dist in zip(indices[0], distances[0]):
            if faiss_idx >= len(self.id_list):
                continue

            emb_id = self.id_list[faiss_idx]
            info = self.embeddings_map[emb_id]

            content = info["content"]
            source = info["source"]
            original_dist = float(dist)

            # ‚ùå Filtrer par cat√©gorie d√©tect√©e
            if "irrelevant" not in categories:
                if source not in categories:
                    continue  # üî• on skip tout ce qui ne correspond pas

            # BOOST sources prioritaires
            boosted = False
            if source in PRIORITY_SOURCES:
                dist *= 0.4
                boosted = True

            scored_results.append((dist, content, source, original_dist, boosted))

            logger.info(
                f" - Embedding ID={emb_id}, src='{source}', "
                f"dist={original_dist:.4f} ‚Üí {dist:.4f}, boosted={boosted}"
            )

        # Tri final
        scored_results.sort(key=lambda x: x[0])

        logger.info("üìä Classement final :")
        for rank, (dist, content, source, original, boosted) in enumerate(scored_results[:top_k], 1):
            logger.info(
                f" #{rank} | source={source} | dist={dist:.4f} | orig={original:.4f} | boosted={boosted}"
            )

        return [content for dist, content, src, orig, boosted in scored_results[:top_k]]

    def ask(self, question: str, top_k: int = 15, llm=None) -> str:
        """
        G√©n√®re la r√©ponse finale avec le contexte filtr√©.
        """
        context_texts = self.search(question, top_k=top_k)
        context = "\n---\n".join(context_texts)

        prompt = f"""
Tu es un assistant sp√©cialis√© dans la connaissance du profil professionnel de **Cl√©ment Gardair**.
Ta mission est de r√©pondre aux questions avec exactitude en utilisant **uniquement le contexte fourni**.
Tu g√©n√®res des r√©ponses fiables, structur√©es, concises et factuelles.

üéØ R√àGLES IMPORTANTES :
- Tu ne dois **jamais inventer** une information qui n'appara√Æt pas dans le contexte.
- Si une information manque, tu dis explicitement : *"Cette information n‚Äôappara√Æt pas dans les donn√©es fournies."*
- Le contexte provient de : mon CV, mes exp√©riences, mes projets GitHub, mes comp√©tences et mes formations.
- Si la question est technique : fournis un exemple de code **court, fonctionnel et pertinent** (mais jamais invent√© si absent du contexte).
- Si la question concerne mon identit√©, parcours ou valeurs personnelles : reste strictement factuel selon les donn√©es disponibles.
- Si le contexte est vide : donne une r√©ponse courte expliquant l'absence d'informations.

üß© CONTEXTE RAG (extraits de mon CV / projets / exp√©riences) :
{context}

‚ùì QUESTION :
{question}

üí¨ R√âPONSE DE L‚ÄôASSISTANT :
"""

        logger.info("üß† Prompt envoy√© au LLM :")
        logger.info(prompt)

        if llm:
            if hasattr(llm, "generate"):
                return llm.generate([{"role": "user", "content": prompt}])
            raise ValueError("Le LLM doit poss√©der une m√©thode `.generate(messages)`")

        return prompt
